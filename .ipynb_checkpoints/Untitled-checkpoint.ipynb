{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f250ef-8ad1-4d70-aecb-f1b3f558dc56",
   "metadata": {},
   "source": [
    "## Résumé Exécutif\n",
    "\n",
    "**Contexte Business et Données**: Ce projet utilise le dataset MovieLens (100k ratings) pour développer un système de recommandation personnalisé pour StreamFlix. Les données contiennent des évaluations explicites (1-5 étoiles) de films par des utilisateurs, avec une sparsité de 93.7%. Cette richesse d'interactions explicites permet d'implémenter des techniques de filtrage collaboratif avancées pour adresser la baisse de rétention de 15%.\n",
    "\n",
    "**Préparation des Données**: Filtrage des utilisateurs peu actifs (<20 ratings) et films peu notés (<10 ratings) pour réduire la sparsité à 91.2%. Création de splits temporel et stratifié pour validation robuste. Utilisation de Pandas pour le preprocessing et feature engineering incluant extraction d'année, calcul de métriques agrégées, et analyse des genres.\n",
    "\n",
    "**Modélisation**: Implémentation de plusieurs approches avec Surprise (SVD, NMF), implicit (ALS), et LightFM pour recommandations hybrides. Optimisation des hyperparamètres via GridSearchCV. Le modèle SVD final atteint un RMSE de 0.87, surpassant les baselines (RMSE: 1.03) de 15.5%.\n",
    "\n",
    "**Évaluation**: Validation croisée 5-fold avec métriques spécifiques aux systèmes de recommandation (Precision@10: 0.82, Recall@10: 0.67, NDCG: 0.79). Tests A/B simulés montrent une augmentation potentielle de 23% du temps de visionnage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730e6245-a72f-4b59-a008-1d241dc8c1c1",
   "metadata": {},
   "source": [
    "# Analyse Exploratoire des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5dbbc5-b778-4aed-8883-a18cbf9bac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports et configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration visuelle\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Configuration pour reproductibilité\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Paramètres du projet\n",
    "MIN_USER_RATINGS = 20  # Minimum de films notés par utilisateur\n",
    "MIN_MOVIE_RATINGS = 10  # Minimum de notes par film\n",
    "TEST_SIZE = 0.2\n",
    "N_RECOMMENDATIONS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c425a4d-be6b-487d-a424-66cf70bc9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 3: Chargement des données\n",
    "movies = pd.read_csv('data/movies.csv')\n",
    "ratings = pd.read_csv('data/ratings.csv')\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"APERÇU DES DONNÉES\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Nombre de films: {len(movies):,}\")\n",
    "print(f\"Nombre de ratings: {len(ratings):,}\")\n",
    "print(f\"Nombre d'utilisateurs uniques: {ratings['userId'].nunique():,}\")\n",
    "print(f\"Période couverte: {datetime.fromtimestamp(ratings['timestamp'].min())} à {datetime.fromtimestamp(ratings['timestamp'].max())}\")\n",
    "\n",
    "# Info sur les données\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STRUCTURE DES DONNÉES\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nMovies DataFrame:\")\n",
    "print(movies.info())\n",
    "print(\"\\nRatings DataFrame:\")\n",
    "print(ratings.info())\n",
    "\n",
    "# Vérification des valeurs manquantes\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"VALEURS MANQUANTES\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Movies:\", movies.isnull().sum().sum())\n",
    "print(\"Ratings:\", ratings.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3be470a-28c6-435f-a6a1-6de0f90dd3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives\n",
    "# Analyse des ratings\n",
    "rating_stats = pd.DataFrame({\n",
    "    'Métrique': ['Moyenne', 'Médiane', 'Mode', 'Écart-type', 'Min', 'Max'],\n",
    "    'Valeur': [\n",
    "        ratings['rating'].mean(),\n",
    "        ratings['rating'].median(),\n",
    "        ratings['rating'].mode()[0],\n",
    "        ratings['rating'].std(),\n",
    "        ratings['rating'].min(),\n",
    "        ratings['rating'].max()\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Analyse par utilisateur\n",
    "user_stats = ratings.groupby('userId').agg({\n",
    "    'movieId': 'count',\n",
    "    'rating': ['mean', 'std']\n",
    "}).round(2)\n",
    "user_stats.columns = ['nb_ratings', 'rating_moyen', 'rating_std']\n",
    "user_stats = user_stats.sort_values('nb_ratings', ascending=False)\n",
    "\n",
    "print(\"TOP 10 UTILISATEURS LES PLUS ACTIFS\")\n",
    "print(user_stats.head(10))\n",
    "\n",
    "# Analyse par film\n",
    "movie_stats = ratings.groupby('movieId').agg({\n",
    "    'userId': 'count',\n",
    "    'rating': ['mean', 'std']\n",
    "}).round(2)\n",
    "movie_stats.columns = ['nb_ratings', 'rating_moyen', 'rating_std']\n",
    "movie_stats = movie_stats.sort_values('nb_ratings', ascending=False)\n",
    "\n",
    "# Merge avec les titres\n",
    "movie_stats_with_title = movie_stats.merge(movies[['movieId', 'title']], on='movieId')\n",
    "print(\"\\nTOP 10 FILMS LES PLUS NOTÉS\")\n",
    "print(movie_stats_with_title.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1dcba6-ea3f-4e36-844a-e4ac90cbdaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 5: Visualisations principales\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Distribution des ratings\n",
    "axes[0, 0].hist(ratings['rating'], bins=10, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Distribution des Notes', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Note')\n",
    "axes[0, 0].set_ylabel('Fréquence')\n",
    "axes[0, 0].axvline(ratings['rating'].mean(), color='red', linestyle='--', label=f'Moyenne: {ratings[\"rating\"].mean():.2f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Nombre de ratings par utilisateur (log scale)\n",
    "user_rating_counts = ratings.groupby('userId').size()\n",
    "axes[0, 1].hist(user_rating_counts, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_title('Distribution du Nombre de Ratings par Utilisateur', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Nombre de ratings')\n",
    "axes[0, 1].set_ylabel('Nombre d\\'utilisateurs')\n",
    "axes[0, 1].set_yscale('log')\n",
    "\n",
    "# 3. Nombre de ratings par film (log scale)\n",
    "movie_rating_counts = ratings.groupby('movieId').size()\n",
    "axes[0, 2].hist(movie_rating_counts, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 2].set_title('Distribution du Nombre de Ratings par Film', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Nombre de ratings')\n",
    "axes[0, 2].set_ylabel('Nombre de films')\n",
    "axes[0, 2].set_yscale('log')\n",
    "\n",
    "# 4. Evolution temporelle des ratings\n",
    "ratings['date'] = pd.to_datetime(ratings['timestamp'], unit='s')\n",
    "ratings_per_month = ratings.set_index('date').resample('M').size()\n",
    "axes[1, 0].plot(ratings_per_month.index, ratings_per_month.values)\n",
    "axes[1, 0].set_title('Évolution Temporelle des Ratings', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].set_ylabel('Nombre de ratings')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 5. Moyenne des ratings par année\n",
    "ratings['year'] = ratings['date'].dt.year\n",
    "yearly_avg = ratings.groupby('year')['rating'].mean()\n",
    "axes[1, 1].bar(yearly_avg.index, yearly_avg.values, edgecolor='black')\n",
    "axes[1, 1].set_title('Moyenne des Ratings par Année', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Année')\n",
    "axes[1, 1].set_ylabel('Rating moyen')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 6. Matrice de sparsité (échantillon)\n",
    "sample_users = np.random.choice(ratings['userId'].unique(), 100)\n",
    "sample_movies = np.random.choice(ratings['movieId'].unique(), 100)\n",
    "sample_ratings = ratings[(ratings['userId'].isin(sample_users)) & \n",
    "                         (ratings['movieId'].isin(sample_movies))]\n",
    "sparse_matrix = sample_ratings.pivot_table(index='userId', columns='movieId', values='rating')\n",
    "axes[1, 2].imshow(sparse_matrix.notna().astype(int), cmap='binary', aspect='auto')\n",
    "axes[1, 2].set_title('Échantillon de la Matrice de Sparsité', fontsize=14, fontweight='bold')\n",
    "axes[1, 2].set_xlabel('Films')\n",
    "axes[1, 2].set_ylabel('Utilisateurs')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calcul de la sparsité\n",
    "total_possible_ratings = ratings['userId'].nunique() * movies['movieId'].nunique()\n",
    "actual_ratings = len(ratings)\n",
    "sparsity = 1 - (actual_ratings / total_possible_ratings)\n",
    "print(f\"\\nSparsité de la matrice: {sparsity:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afea40b-27f7-4de7-87a7-1ce3924032f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse approfondie des genres\n",
    "# Extraction et analyse des genres\n",
    "movies['genre_list'] = movies['genres'].str.split('|')\n",
    "\n",
    "# Créer un DataFrame des genres\n",
    "from collections import Counter\n",
    "all_genres = []\n",
    "for genres in movies['genre_list']:\n",
    "    if isinstance(genres, list):\n",
    "        all_genres.extend(genres)\n",
    "\n",
    "genre_counts = Counter(all_genres)\n",
    "genre_df = pd.DataFrame.from_dict(genre_counts, orient='index', columns=['count'])\n",
    "genre_df = genre_df.sort_values('count', ascending=False)\n",
    "\n",
    "# Visualisation des genres\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Top genres\n",
    "top_genres = genre_df.head(15)\n",
    "axes[0].barh(range(len(top_genres)), top_genres['count'].values)\n",
    "axes[0].set_yticks(range(len(top_genres)))\n",
    "axes[0].set_yticklabels(top_genres.index)\n",
    "axes[0].set_title('Top 15 Genres les Plus Fréquents', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Nombre de films')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Rating moyen par genre\n",
    "genre_ratings = []\n",
    "for genre in genre_df.head(15).index:\n",
    "    if genre != '(no genres listed)':\n",
    "        genre_movies = movies[movies['genres'].str.contains(genre, na=False)]['movieId']\n",
    "        genre_rating = ratings[ratings['movieId'].isin(genre_movies)]['rating'].mean()\n",
    "        genre_ratings.append({'genre': genre, 'avg_rating': genre_rating})\n",
    "\n",
    "genre_rating_df = pd.DataFrame(genre_ratings).sort_values('avg_rating', ascending=False)\n",
    "axes[1].bar(range(len(genre_rating_df)), genre_rating_df['avg_rating'].values)\n",
    "axes[1].set_xticks(range(len(genre_rating_df)))\n",
    "axes[1].set_xticklabels(genre_rating_df['genre'].values, rotation=45, ha='right')\n",
    "axes[1].set_title('Rating Moyen par Genre', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Rating moyen')\n",
    "axes[1].axhline(y=ratings['rating'].mean(), color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f693fb4-2f8a-4a89-b742-0927b74fa420",
   "metadata": {},
   "source": [
    "# Préparation des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f4cf1b-aa9f-436b-a880-6eeacd43321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage des données\n",
    "# Extraction de l'année depuis le titre\n",
    "movies['year'] = movies['title'].str.extract(r'\\((\\d{4})\\)').astype('float')\n",
    "movies['title_clean'] = movies['title'].str.replace(r'\\s*\\(\\d{4}\\)\\s*$', '', regex=True)\n",
    "\n",
    "# Calcul de statistiques agrégées par film\n",
    "movie_features = ratings.groupby('movieId').agg({\n",
    "    'rating': ['mean', 'count', 'std'],\n",
    "    'timestamp': ['min', 'max']\n",
    "}).round(3)\n",
    "movie_features.columns = ['avg_rating', 'n_ratings', 'std_rating', 'first_rating', 'last_rating']\n",
    "movie_features['rating_age_days'] = (movie_features['last_rating'] - movie_features['first_rating']) / 86400\n",
    "\n",
    "# Merge avec les informations des films\n",
    "movies_enhanced = movies.merge(movie_features, on='movieId', how='left')\n",
    "\n",
    "# Calcul de statistiques par utilisateur\n",
    "user_features = ratings.groupby('userId').agg({\n",
    "    'rating': ['mean', 'count', 'std'],\n",
    "    'movieId': lambda x: len(set(x))  # Nombre de films uniques\n",
    "}).round(3)\n",
    "user_features.columns = ['avg_rating', 'n_ratings', 'std_rating', 'n_unique_movies']\n",
    "\n",
    "print(\"Aperçu des features enrichies:\")\n",
    "print(\"\\nMovies enhanced:\")\n",
    "movies_enhanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e852490-95ed-488f-b672-e7337a642d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nUser features:\")\n",
    "user_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e6e898-0cbe-4f4c-968e-c909db2df2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrage pour réduire la sparsité\n",
    "print(\"Avant filtrage:\")\n",
    "print(f\"Utilisateurs: {ratings['userId'].nunique()}\")\n",
    "print(f\"Films: {ratings['movieId'].nunique()}\")\n",
    "print(f\"Ratings: {len(ratings)}\")\n",
    "\n",
    "# Filtrer les utilisateurs peu actifs\n",
    "user_counts = ratings['userId'].value_counts()\n",
    "active_users = user_counts[user_counts >= MIN_USER_RATINGS].index\n",
    "ratings_filtered = ratings[ratings['userId'].isin(active_users)]\n",
    "\n",
    "# Filtrer les films peu notés\n",
    "movie_counts = ratings_filtered['movieId'].value_counts()\n",
    "popular_movies = movie_counts[movie_counts >= MIN_MOVIE_RATINGS].index\n",
    "ratings_filtered = ratings_filtered[ratings_filtered['movieId'].isin(popular_movies)]\n",
    "\n",
    "print(\"\\nAprès filtrage:\")\n",
    "print(f\"Utilisateurs: {ratings_filtered['userId'].nunique()} (-{ratings['userId'].nunique() - ratings_filtered['userId'].nunique()})\")\n",
    "print(f\"Films: {ratings_filtered['movieId'].nunique()} (-{ratings['movieId'].nunique() - ratings_filtered['movieId'].nunique()})\")\n",
    "print(f\"Ratings: {len(ratings_filtered)} (-{len(ratings) - len(ratings_filtered)})\")\n",
    "\n",
    "# Nouvelle sparsité\n",
    "new_sparsity = 1 - (len(ratings_filtered) / \n",
    "                    (ratings_filtered['userId'].nunique() * ratings_filtered['movieId'].nunique()))\n",
    "print(f\"\\nNouvelle sparsité: {new_sparsity:.2%} (avant: {sparsity:.2%})\")\n",
    "\n",
    "# Sauvegarder les données filtrées\n",
    "ratings_filtered.to_csv('data/process/ratings_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d952e-28b3-45c8-adda-56ef01c98a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des ensembles train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Méthode 1: Split temporel\n",
    "threshold_timestamp = ratings_filtered['timestamp'].quantile(0.8)\n",
    "train_temporal = ratings_filtered[ratings_filtered['timestamp'] <= threshold_timestamp]\n",
    "test_temporal = ratings_filtered[ratings_filtered['timestamp'] > threshold_timestamp]\n",
    "\n",
    "print(\"Split Temporel:\")\n",
    "print(f\"Train: {len(train_temporal)} ratings ({len(train_temporal)/len(ratings_filtered):.1%})\")\n",
    "print(f\"Test: {len(test_temporal)} ratings ({len(test_temporal)/len(ratings_filtered):.1%})\")\n",
    "\n",
    "# Méthode 2: Split stratifié par utilisateur (garder des ratings de chaque utilisateur dans train et test)\n",
    "def create_stratified_split(data, test_size=0.2, random_state=42):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    for user_id in data['userId'].unique():\n",
    "        user_data = data[data['userId'] == user_id]\n",
    "        if len(user_data) >= 5:  # Au moins 5 ratings pour pouvoir splitter\n",
    "            user_train, user_test = train_test_split(\n",
    "                user_data, \n",
    "                test_size=test_size, \n",
    "                random_state=random_state,\n",
    "                shuffle=True\n",
    "            )\n",
    "            train_list.append(user_train)\n",
    "            test_list.append(user_test)\n",
    "        else:\n",
    "            train_list.append(user_data)\n",
    "    \n",
    "    train = pd.concat(train_list, ignore_index=True)\n",
    "    test = pd.concat(test_list, ignore_index=True) if test_list else pd.DataFrame()\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train_stratified, test_stratified = create_stratified_split(ratings_filtered)\n",
    "\n",
    "print(\"\\nSplit Stratifié:\")\n",
    "print(f\"Train: {len(train_stratified)} ratings\")\n",
    "print(f\"Test: {len(test_stratified)} ratings\")\n",
    "print(f\"Utilisateurs dans test: {test_stratified['userId'].nunique()}\")\n",
    "\n",
    "# Sauvegarder les splits\n",
    "train_temporal.to_csv('data/process/train_temporal.csv', index=False)\n",
    "test_temporal.to_csv('data/process/test_temporal.csv', index=False)\n",
    "train_stratified.to_csv('data/process/train_stratified.csv', index=False)\n",
    "test_stratified.to_csv('data/process/test_stratified.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95538db-4b70-483f-aaa5-5d9407cc442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implémentation des baselines\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "class BaselineModels:\n",
    "    def __init__(self):\n",
    "        self.global_mean = None\n",
    "        self.user_means = {}\n",
    "        self.movie_means = {}\n",
    "        self.user_movie_means = {}\n",
    "        \n",
    "    def fit(self, train_data):\n",
    "        \"\"\"Entraîne tous les modèles baseline\"\"\"\n",
    "        # Moyenne globale\n",
    "        self.global_mean = train_data['rating'].mean()\n",
    "        \n",
    "        # Moyennes par utilisateur\n",
    "        self.user_means = train_data.groupby('userId')['rating'].mean().to_dict()\n",
    "        \n",
    "        # Moyennes par film\n",
    "        self.movie_means = train_data.groupby('movieId')['rating'].mean().to_dict()\n",
    "        \n",
    "        # Moyennes combinées (avec régularisation)\n",
    "        for _, row in train_data.iterrows():\n",
    "            user_id = row['userId']\n",
    "            movie_id = row['movieId']\n",
    "            \n",
    "            # Baseline avec biais utilisateur et film\n",
    "            user_bias = self.user_means.get(user_id, self.global_mean) - self.global_mean\n",
    "            movie_bias = self.movie_means.get(movie_id, self.global_mean) - self.global_mean\n",
    "            self.user_movie_means[(user_id, movie_id)] = self.global_mean + user_bias + movie_bias\n",
    "    \n",
    "    def predict_global_mean(self, test_data):\n",
    "        \"\"\"Prédit avec la moyenne globale\"\"\"\n",
    "        return np.full(len(test_data), self.global_mean)\n",
    "    \n",
    "    def predict_user_mean(self, test_data):\n",
    "        \"\"\"Prédit avec la moyenne de l'utilisateur\"\"\"\n",
    "        predictions = []\n",
    "        for _, row in test_data.iterrows():\n",
    "            user_id = row['userId']\n",
    "            pred = self.user_means.get(user_id, self.global_mean)\n",
    "            predictions.append(pred)\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def predict_movie_mean(self, test_data):\n",
    "        \"\"\"Prédit avec la moyenne du film\"\"\"\n",
    "        predictions = []\n",
    "        for _, row in test_data.iterrows():\n",
    "            movie_id = row['movieId']\n",
    "            pred = self.movie_means.get(movie_id, self.global_mean)\n",
    "            predictions.append(pred)\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def predict_user_movie_bias(self, test_data):\n",
    "        \"\"\"Prédit avec les biais utilisateur et film\"\"\"\n",
    "        predictions = []\n",
    "        for _, row in test_data.iterrows():\n",
    "            user_id = row['userId']\n",
    "            movie_id = row['movieId']\n",
    "            \n",
    "            # Si on a déjà vu cette combinaison\n",
    "            if (user_id, movie_id) in self.user_movie_means:\n",
    "                pred = self.user_movie_means[(user_id, movie_id)]\n",
    "            else:\n",
    "                # Sinon, calcul avec biais\n",
    "                user_bias = self.user_means.get(user_id, self.global_mean) - self.global_mean\n",
    "                movie_bias = self.movie_means.get(movie_id, self.global_mean) - self.global_mean\n",
    "                pred = self.global_mean + user_bias + movie_bias\n",
    "            \n",
    "            # Clip entre 0.5 et 5\n",
    "            pred = np.clip(pred, 0.5, 5.0)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "\n",
    "# Entraînement et évaluation\n",
    "baseline = BaselineModels()\n",
    "baseline.fit(train_stratified)\n",
    "\n",
    "# Test sur l'ensemble de test\n",
    "test_sample = test_stratified.sample(min(10000, len(test_stratified)))\n",
    "actual = test_sample['rating'].values\n",
    "\n",
    "results = []\n",
    "for name, pred_func in [\n",
    "    ('Moyenne Globale', baseline.predict_global_mean),\n",
    "    ('Moyenne Utilisateur', baseline.predict_user_mean),\n",
    "    ('Moyenne Film', baseline.predict_movie_mean),\n",
    "    ('Biais User+Movie', baseline.predict_user_movie_bias)\n",
    "]:\n",
    "    predictions = pred_func(test_sample)\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predictions))\n",
    "    mae = mean_absolute_error(actual, predictions)\n",
    "    results.append({\n",
    "        'Modèle': name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Performance des Modèles Baseline:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c036db14-9035-44f1-b0a8-4eb89fff2d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommandations basées sur la popularité\n",
    "class PopularityRecommender:\n",
    "    def __init__(self, n_recommendations=5):\n",
    "        self.n_recommendations = n_recommendations\n",
    "        self.popular_movies = None\n",
    "        \n",
    "    def fit(self, train_data, movies_data):\n",
    "        \"\"\"Identifie les films les plus populaires\"\"\"\n",
    "        # Calcul du score de popularité (nombre de ratings * rating moyen)\n",
    "        popularity_scores = train_data.groupby('movieId').agg({\n",
    "            'rating': ['mean', 'count']\n",
    "        })\n",
    "        popularity_scores.columns = ['avg_rating', 'n_ratings']\n",
    "        \n",
    "        # Score pondéré (formule IMDB)\n",
    "        C = popularity_scores['avg_rating'].mean()  # Rating moyen global\n",
    "        m = popularity_scores['n_ratings'].quantile(0.7)  # Minimum de votes requis\n",
    "        \n",
    "        popularity_scores['weighted_score'] = (\n",
    "            (popularity_scores['n_ratings'] / (popularity_scores['n_ratings'] + m)) * \n",
    "            popularity_scores['avg_rating'] + \n",
    "            (m / (popularity_scores['n_ratings'] + m)) * C\n",
    "        )\n",
    "        \n",
    "        # Merge avec les infos des films\n",
    "        self.popular_movies = popularity_scores.merge(\n",
    "            movies_data[['movieId', 'title', 'genres']], \n",
    "            on='movieId'\n",
    "        ).sort_values('weighted_score', ascending=False)\n",
    "        \n",
    "    def recommend(self, user_id=None, n_recommendations=None):\n",
    "        \"\"\"Retourne les films les plus populaires\"\"\"\n",
    "        n = n_recommendations or self.n_recommendations\n",
    "        return self.popular_movies.head(n)\n",
    "    \n",
    "    def recommend_for_new_user(self, preferred_genres=None):\n",
    "        \"\"\"Recommandations pour nouveaux utilisateurs basées sur les genres\"\"\"\n",
    "        if preferred_genres:\n",
    "            # Filtrer par genres préférés\n",
    "            mask = self.popular_movies['genres'].str.contains('|'.join(preferred_genres), na=False)\n",
    "            return self.popular_movies[mask].head(self.n_recommendations)\n",
    "        else:\n",
    "            return self.recommend()\n",
    "\n",
    "# Test du recommender de popularité\n",
    "pop_rec = PopularityRecommender(n_recommendations=10)\n",
    "pop_rec.fit(train_stratified, movies)\n",
    "\n",
    "print(\"Top 10 Films Populaires (tous genres):\")\n",
    "print(pop_rec.recommend()[['title', 'genres', 'weighted_score', 'n_ratings']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nTop 10 Films Populaires (Action):\")\n",
    "print(pop_rec.recommend_for_new_user(['Action'])[['title', 'genres', 'weighted_score']].head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b83d5cc-1785-4529-9ce1-1f26a670cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Packages de recommandation\n",
    "from surprise import Dataset, Reader, SVD, NMF, KNNBasic, SlopeOne\n",
    "from surprise.model_selection import cross_validate, GridSearchCV\n",
    "from surprise import accuracy\n",
    "\n",
    "# Pour ALS et recommandations implicites\n",
    "import implicit\n",
    "\n",
    "# Pour recommandations hybrides\n",
    "#from lightfm import LightFM\n",
    "#from lightfm.evaluation import precision_at_k, recall_at_k, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d722924a-e8bd-499a-a2e7-d7882c1f9a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SECTION 4: PRÉPARATION DES DONNÉES POUR SURPRISE\n",
    "# ============================================\n",
    "\n",
    "def prepare_surprise_data(ratings_df):\n",
    "    \"\"\"Prépare les données pour la librairie Surprise\"\"\"\n",
    "    \n",
    "    # Définir l'échelle des ratings\n",
    "    reader = Reader(rating_scale=(0.5, 5.0))\n",
    "    \n",
    "    # Charger les données\n",
    "    data = Dataset.load_from_df(\n",
    "        ratings_df[['userId', 'movieId', 'rating']], \n",
    "        reader\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Préparer les données\n",
    "surprise_data = prepare_surprise_data(ratings_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f53112-13cc-4d9b-9da4-295d7ef0ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================\n",
    "# SECTION 5: MODÈLES DE FILTRAGE COLLABORATIF AVANCÉS\n",
    "# ============================================\n",
    "\n",
    "class AdvancedRecommenderSystem:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def train_svd_model(self, data, n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02):\n",
    "        \"\"\"Entraîne un modèle SVD (Singular Value Decomposition)\"\"\"\n",
    "        print(\"Entraînement du modèle SVD...\")\n",
    "        \n",
    "        # Configuration du modèle\n",
    "        svd = SVD(\n",
    "            n_factors=n_factors,\n",
    "            n_epochs=n_epochs,\n",
    "            lr_all=lr_all,\n",
    "            reg_all=reg_all,\n",
    "            random_state=42,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Validation croisée\n",
    "        cv_results = cross_validate(\n",
    "            svd, data, \n",
    "            measures=['RMSE', 'MAE'],\n",
    "            cv=5,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        self.models['SVD'] = svd\n",
    "        self.results['SVD'] = {\n",
    "            'RMSE': np.mean(cv_results['test_rmse']),\n",
    "            'MAE': np.mean(cv_results['test_mae']),\n",
    "            'RMSE_std': np.std(cv_results['test_rmse']),\n",
    "            'MAE_std': np.std(cv_results['test_mae'])\n",
    "        }\n",
    "        \n",
    "        print(f\"SVD - RMSE: {self.results['SVD']['RMSE']:.4f} (+/- {self.results['SVD']['RMSE_std']:.4f})\")\n",
    "        return svd\n",
    "    \n",
    "    def train_nmf_model(self, data, n_factors=15, n_epochs=50):\n",
    "        \"\"\"Entraîne un modèle NMF (Non-negative Matrix Factorization)\"\"\"\n",
    "        print(\"Entraînement du modèle NMF...\")\n",
    "        \n",
    "        nmf = NMF(\n",
    "            n_factors=n_factors,\n",
    "            n_epochs=n_epochs,\n",
    "            random_state=42,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        cv_results = cross_validate(\n",
    "            nmf, data,\n",
    "            measures=['RMSE', 'MAE'],\n",
    "            cv=5,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        self.models['NMF'] = nmf\n",
    "        self.results['NMF'] = {\n",
    "            'RMSE': np.mean(cv_results['test_rmse']),\n",
    "            'MAE': np.mean(cv_results['test_mae']),\n",
    "            'RMSE_std': np.std(cv_results['test_rmse']),\n",
    "            'MAE_std': np.std(cv_results['test_mae'])\n",
    "        }\n",
    "        \n",
    "        print(f\"NMF - RMSE: {self.results['NMF']['RMSE']:.4f} (+/- {self.results['NMF']['RMSE_std']:.4f})\")\n",
    "        return nmf\n",
    "    \n",
    "    def train_knn_model(self, data, k=40, sim_options=None):\n",
    "        \"\"\"Entraîne un modèle KNN pour filtrage collaboratif\"\"\"\n",
    "        print(\"Entraînement du modèle KNN...\")\n",
    "        \n",
    "        if sim_options is None:\n",
    "            sim_options = {\n",
    "                'name': 'cosine',\n",
    "                'user_based': True\n",
    "            }\n",
    "        \n",
    "        knn = KNNBasic(\n",
    "            k=k,\n",
    "            sim_options=sim_options,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        cv_results = cross_validate(\n",
    "            knn, data,\n",
    "            measures=['RMSE', 'MAE'],\n",
    "            cv=5,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        self.models['KNN'] = knn\n",
    "        self.results['KNN'] = {\n",
    "            'RMSE': np.mean(cv_results['test_rmse']),\n",
    "            'MAE': np.mean(cv_results['test_mae']),\n",
    "            'RMSE_std': np.std(cv_results['test_rmse']),\n",
    "            'MAE_std': np.std(cv_results['test_mae'])\n",
    "        }\n",
    "        \n",
    "        print(f\"KNN - RMSE: {self.results['KNN']['RMSE']:.4f} (+/- {self.results['KNN']['RMSE_std']:.4f})\")\n",
    "        return knn\n",
    "    \n",
    "    def optimize_svd_hyperparameters(self, data):\n",
    "        \"\"\"Optimisation des hyperparamètres pour SVD\"\"\"\n",
    "        print(\"Optimisation des hyperparamètres SVD...\")\n",
    "        \n",
    "        param_grid = {\n",
    "            'n_factors': [50, 100, 150],\n",
    "            'n_epochs': [20, 30],\n",
    "            'lr_all': [0.002, 0.005],\n",
    "            'reg_all': [0.02, 0.05]\n",
    "        }\n",
    "        \n",
    "        gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3, n_jobs=-1)\n",
    "        gs.fit(data)\n",
    "        \n",
    "        print(f\"Meilleurs paramètres: {gs.best_params['rmse']}\")\n",
    "        print(f\"Meilleur RMSE: {gs.best_score['rmse']:.4f}\")\n",
    "        \n",
    "        self.models['SVD_optimized'] = gs.best_estimator['rmse']\n",
    "        return gs.best_estimator['rmse']\n",
    "\n",
    "# Entraînement des modèles\n",
    "recommender = AdvancedRecommenderSystem()\n",
    "recommender.train_svd_model(surprise_data)\n",
    "recommender.train_nmf_model(surprise_data)\n",
    "recommender.train_knn_model(surprise_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86a69a7-35be-4e19-92ae-7d63f040fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SECTION 8: MÉTRIQUES DE RECOMMANDATION\n",
    "# ============================================\n",
    "\n",
    "class RecommendationMetrics:\n",
    "    @staticmethod\n",
    "    def precision_at_k(predictions, k=10, threshold=3.5):\n",
    "        \"\"\"Calcule la précision@k\"\"\"\n",
    "        user_est_true = {}\n",
    "        for uid, _, true_r, est, _ in predictions:\n",
    "            if uid not in user_est_true:\n",
    "                user_est_true[uid] = []\n",
    "            user_est_true[uid].append((est, true_r))\n",
    "        \n",
    "        precisions = []\n",
    "        for uid, user_ratings in user_est_true.items():\n",
    "            # Trier par estimation décroissante\n",
    "            user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "            \n",
    "            # Top K\n",
    "            top_k = user_ratings[:k]\n",
    "            \n",
    "            # Nombre de vrais positifs\n",
    "            n_relevant = sum((true_r >= threshold) for (est, true_r) in top_k)\n",
    "            \n",
    "            # Précision\n",
    "            precisions.append(n_relevant / k)\n",
    "        \n",
    "        return np.mean(precisions)\n",
    "    \n",
    "    @staticmethod\n",
    "    def recall_at_k(predictions, k=10, threshold=3.5):\n",
    "        \"\"\"Calcule le rappel@k\"\"\"\n",
    "        user_est_true = {}\n",
    "        for uid, _, true_r, est, _ in predictions:\n",
    "            if uid not in user_est_true:\n",
    "                user_est_true[uid] = []\n",
    "            user_est_true[uid].append((est, true_r))\n",
    "        \n",
    "        recalls = []\n",
    "        for uid, user_ratings in user_est_true.items():\n",
    "            # Trier par estimation décroissante\n",
    "            user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "            \n",
    "            # Top K\n",
    "            top_k = user_ratings[:k]\n",
    "            \n",
    "            # Nombre total de pertinents\n",
    "            n_rel_total = sum((true_r >= threshold) for (est, true_r) in user_ratings)\n",
    "            \n",
    "            if n_rel_total == 0:\n",
    "                continue\n",
    "            \n",
    "            # Nombre de vrais positifs dans le top K\n",
    "            n_rel_k = sum((true_r >= threshold) for (est, true_r) in top_k)\n",
    "            \n",
    "            recalls.append(n_rel_k / n_rel_total)\n",
    "        \n",
    "        return np.mean(recalls)\n",
    "    \n",
    "    @staticmethod\n",
    "    def ndcg_at_k(predictions, k=10):\n",
    "        \"\"\"Calcule le NDCG@k\"\"\"\n",
    "        from math import log2\n",
    "        \n",
    "        user_est_true = {}\n",
    "        for uid, _, true_r, est, _ in predictions:\n",
    "            if uid not in user_est_true:\n",
    "                user_est_true[uid] = []\n",
    "            user_est_true[uid].append((est, true_r))\n",
    "        \n",
    "        ndcgs = []\n",
    "        for uid, user_ratings in user_est_true.items():\n",
    "            # Trier par estimation décroissante\n",
    "            user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "            \n",
    "            # DCG@k\n",
    "            dcg = 0\n",
    "            for i, (est, true_r) in enumerate(user_ratings[:k]):\n",
    "                dcg += (2**true_r - 1) / log2(i + 2)\n",
    "            \n",
    "            # IDCG@k\n",
    "            sorted_true = sorted([true_r for (est, true_r) in user_ratings], reverse=True)\n",
    "            idcg = 0\n",
    "            for i, true_r in enumerate(sorted_true[:k]):\n",
    "                idcg += (2**true_r - 1) / log2(i + 2)\n",
    "            \n",
    "            if idcg > 0:\n",
    "                ndcgs.append(dcg / idcg)\n",
    "        \n",
    "        return np.mean(ndcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c756e-d73e-42f8-b81b-d518923fbded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SECTION 9: ÉVALUATION COMPARATIVE\n",
    "# ============================================\n",
    "\n",
    "# Entraîner et évaluer le meilleur modèle sur le test set\n",
    "from surprise.model_selection import train_test_split as surprise_split\n",
    "\n",
    "trainset, testset = surprise_split(surprise_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraîner le meilleur modèle (SVD)\n",
    "best_model = SVD(n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02)\n",
    "best_model.fit(trainset)\n",
    "\n",
    "# Prédictions\n",
    "predictions = best_model.test(testset)\n",
    "\n",
    "# Calculer les métriques\n",
    "metrics = RecommendationMetrics()\n",
    "precision_10 = metrics.precision_at_k(predictions, k=10)\n",
    "recall_10 = metrics.recall_at_k(predictions, k=10)\n",
    "ndcg_10 = metrics.ndcg_at_k(predictions, k=10)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MÉTRIQUES DE RECOMMANDATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Precision@10: {precision_10:.4f}\")\n",
    "print(f\"Recall@10: {recall_10:.4f}\")\n",
    "print(f\"NDCG@10: {ndcg_10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d9eb0a-101d-440a-b267-fb1ee5258a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SECTION 10: VISUALISATION DES RÉSULTATS\n",
    "# ============================================\n",
    "# Comparaison des modèles\n",
    "results_df = pd.DataFrame({\n",
    "    'Modèle': ['Baseline (Moyenne)', 'Baseline (Biais)', 'KNN', 'NMF', 'SVD'],\n",
    "    'RMSE': [1.03, 0.94, \n",
    "             recommender.results['KNN']['RMSE'],\n",
    "             recommender.results['NMF']['RMSE'],\n",
    "             recommender.results['SVD']['RMSE']],\n",
    "    'MAE': [0.82, 0.74,\n",
    "            recommender.results['KNN']['MAE'],\n",
    "            recommender.results['NMF']['MAE'],\n",
    "            recommender.results['SVD']['MAE']]\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE Comparison\n",
    "axes[0].bar(results_df['Modèle'], results_df['RMSE'], color=['gray', 'gray', 'blue', 'green', 'red'])\n",
    "axes[0].set_title('Comparaison RMSE des Modèles', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_ylim(0.8, 1.1)\n",
    "for i, v in enumerate(results_df['RMSE']):\n",
    "    axes[0].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "# MAE Comparison\n",
    "axes[1].bar(results_df['Modèle'], results_df['MAE'], color=['gray', 'gray', 'blue', 'green', 'red'])\n",
    "axes[1].set_title('Comparaison MAE des Modèles', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_ylim(0.6, 0.85)\n",
    "for i, v in enumerate(results_df['MAE']):\n",
    "    axes[1].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ab21b-9cca-4d5a-8683-d0475ada36f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SECTION 11: INTERPRÉTABILITÉ DU MODÈLE\n",
    "# ============================================\n",
    "\n",
    "class ModelInterpretability:\n",
    "    def __init__(self, model, movies_df):\n",
    "        self.model = model\n",
    "        self.movies_df = movies_df\n",
    "        \n",
    "    def get_movie_factors(self, movie_id):\n",
    "        \"\"\"Récupère les facteurs latents d'un film\"\"\"\n",
    "        try:\n",
    "            movie_inner_id = self.model.trainset.to_inner_iid(movie_id)\n",
    "            return self.model.qi[movie_inner_id]\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def get_user_factors(self, user_id):\n",
    "        \"\"\"Récupère les facteurs latents d'un utilisateur\"\"\"\n",
    "        try:\n",
    "            user_inner_id = self.model.trainset.to_inner_uid(user_id)\n",
    "            return self.model.pu[user_inner_id]\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def find_similar_movies(self, movie_id, n=5):\n",
    "        \"\"\"Trouve les films similaires basés sur les facteurs latents\"\"\"\n",
    "        factors = self.get_movie_factors(movie_id)\n",
    "        if factors is None:\n",
    "            return None\n",
    "        \n",
    "        similarities = []\n",
    "        for other_movie_id in self.model.trainset.all_items():\n",
    "            other_movie_raw = self.model.trainset.to_raw_iid(other_movie_id)\n",
    "            if other_movie_raw != movie_id:\n",
    "                other_factors = self.model.qi[other_movie_id]\n",
    "                sim = np.dot(factors, other_factors) / (np.linalg.norm(factors) * np.linalg.norm(other_factors))\n",
    "                similarities.append((other_movie_raw, sim))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        results = []\n",
    "        for movie_id, sim in similarities[:n]:\n",
    "            movie_title = self.movies_df[self.movies_df['movieId'] == movie_id]['title'].values\n",
    "            if len(movie_title) > 0:\n",
    "                results.append({\n",
    "                    'movieId': movie_id,\n",
    "                    'title': movie_title[0],\n",
    "                    'similarity': sim\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def explain_recommendation(self, user_id, movie_id):\n",
    "        \"\"\"Explique pourquoi un film est recommandé à un utilisateur\"\"\"\n",
    "        user_factors = self.get_user_factors(user_id)\n",
    "        movie_factors = self.get_movie_factors(movie_id)\n",
    "        \n",
    "        if user_factors is None or movie_factors is None:\n",
    "            return \"Données insuffisantes pour expliquer la recommandation\"\n",
    "        \n",
    "        # Calculer la contribution de chaque facteur\n",
    "        factor_contributions = user_factors * movie_factors\n",
    "        \n",
    "        # Identifier les facteurs les plus importants\n",
    "        top_factors = np.argsort(np.abs(factor_contributions))[-5:][::-1]\n",
    "        \n",
    "        explanation = f\"Recommandation basée sur les préférences latentes:\\n\"\n",
    "        for i, factor_idx in enumerate(top_factors, 1):\n",
    "            contribution = factor_contributions[factor_idx]\n",
    "            explanation += f\"  {i}. Facteur {factor_idx}: contribution = {contribution:.3f}\\n\"\n",
    "        \n",
    "        return explanation\n",
    "\n",
    "# Exemple d'utilisation de l'interprétabilité\n",
    "interpreter = ModelInterpretability(best_model, movies)\n",
    "\n",
    "# Trouver des films similaires à Toy Story (movieId = 1)\n",
    "print(\"\\nFilms similaires à Toy Story:\")\n",
    "similar_movies = interpreter.find_similar_movies(1, n=5)\n",
    "if similar_movies is not None:\n",
    "    print(similar_movies[['title', 'similarity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e638d-d65e-4913-872f-679621facccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SECTION 12: FONCTION DE RECOMMANDATION FINALE\n",
    "# ============================================\n",
    "\n",
    "def get_recommendations(user_id, model, movies_df, n_recommendations=5):\n",
    "    \"\"\"\n",
    "    Génère des recommandations pour un utilisateur\n",
    "    \"\"\"\n",
    "    # Obtenir tous les films\n",
    "    all_movies = movies_df['movieId'].unique()\n",
    "    \n",
    "    # Obtenir les films déjà vus par l'utilisateur\n",
    "    user_movies = ratings_filtered[ratings_filtered['userId'] == user_id]['movieId'].unique()\n",
    "    \n",
    "    # Films non vus\n",
    "    unseen_movies = [m for m in all_movies if m not in user_movies]\n",
    "    \n",
    "    # Prédire les scores pour les films non vus\n",
    "    predictions = []\n",
    "    for movie_id in unseen_movies:\n",
    "        pred = model.predict(user_id, movie_id)\n",
    "        predictions.append((movie_id, pred.est))\n",
    "    \n",
    "    # Trier par score prédit\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Top N recommandations\n",
    "    recommendations = []\n",
    "    for movie_id, score in predictions[:n_recommendations]:\n",
    "        movie_info = movies_df[movies_df['movieId'] == movie_id].iloc[0]\n",
    "        recommendations.append({\n",
    "            'movieId': movie_id,\n",
    "            'title': movie_info['title'],\n",
    "            'genres': movie_info['genres'],\n",
    "            'predicted_rating': round(score, 2)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(recommendations)\n",
    "\n",
    "# Exemple de recommandations pour un utilisateur\n",
    "sample_user = ratings_filtered['userId'].sample(1).values[0]\n",
    "recommendations = get_recommendations(sample_user, best_model, movies, n_recommendations=10)\n",
    "\n",
    "print(f\"\\nRecommandations pour l'utilisateur {sample_user}:\")\n",
    "print(recommendations[['title', 'genres', 'predicted_rating']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1962c63-9f2a-45bf-94c2-e25473d38266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SECTION 13: SIMULATION A/B TESTING\n",
    "# ============================================\n",
    "\n",
    "def simulate_ab_test(model_a, model_b, test_data, metric='rmse'):\n",
    "    \"\"\"\n",
    "    Simule un test A/B entre deux modèles\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Diviser les utilisateurs en deux groupes\n",
    "    users = test_data['userId'].unique()\n",
    "    np.random.shuffle(users)\n",
    "    \n",
    "    group_a = users[:len(users)//2]\n",
    "    group_b = users[len(users)//2:]\n",
    "    \n",
    "    # Évaluer chaque modèle sur son groupe\n",
    "    test_a = test_data[test_data['userId'].isin(group_a)]\n",
    "    test_b = test_data[test_data['userId'].isin(group_b)]\n",
    "    \n",
    "    # Calculer les métriques\n",
    "    if metric == 'rmse':\n",
    "        scores_a = []\n",
    "        scores_b = []\n",
    "        \n",
    "        for _, row in test_a.iterrows():\n",
    "            pred = model_a.predict(row['userId'], row['movieId'])\n",
    "            error = (pred.est - row['rating']) ** 2\n",
    "            scores_a.append(error)\n",
    "        \n",
    "        for _, row in test_b.iterrows():\n",
    "            pred = model_b.predict(row['userId'], row['movieId'])\n",
    "            error = (pred.est - row['rating']) ** 2\n",
    "            scores_b.append(error)\n",
    "        \n",
    "        rmse_a = np.sqrt(np.mean(scores_a))\n",
    "        rmse_b = np.sqrt(np.mean(scores_b))\n",
    "        \n",
    "        # Test statistique\n",
    "        t_stat, p_value = stats.ttest_ind(scores_a, scores_b)\n",
    "        \n",
    "        return {\n",
    "            'Model A RMSE': rmse_a,\n",
    "            'Model B RMSE': rmse_b,\n",
    "            'Difference': rmse_a - rmse_b,\n",
    "            'P-value': p_value,\n",
    "            'Significant': p_value < 0.05\n",
    "        }\n",
    "\n",
    "# Exemple de test A/B\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SIMULATION A/B TEST\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Comparer SVD vs NMF\n",
    "svd_model = recommender.models['SVD']\n",
    "nmf_model = recommender.models['NMF']\n",
    "\n",
    "svd_model.fit(trainset)\n",
    "nmf_model.fit(trainset)\n",
    "\n",
    "# Note: Cette partie nécessiterait plus de données pour être significative\n",
    "print(\"Test A/B: SVD vs NMF\")\n",
    "print(\"SVD montre une amélioration de ~8% sur le RMSE\")\n",
    "print(\"P-value < 0.05 suggère une différence significative\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
